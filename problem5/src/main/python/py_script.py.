from pyspark import SparkContext,SparkConf
from pyspark.sql import HiveContext,SQLContext


if __name__ == '__main__':
	# Create spark configuration and context	
	conf = SparkConf().setAppName("BaseBall Stastistics").setMaster("local[*]")
	sc = SparkContext(conf = conf)

	# read input files
	f1 = sc.textFile("flightdelays/fldata1.csv")
	f2 = sc.textFile("flightdelays/fldata2.csv")
	f3 = sc.textFile("flightdelays/fldata3.csv")

	# extract header for later filtering.
	header = f1.first()
	
	# TASK 02 : Clean data 

	'''
		extracted records :Year, Month, DayofMonth, DepTime, UniqueCarrier, 
				   FlightNum, ArrDelay, Origin and Dest
	'''

	f1m = f1.filter(lambda x : (x != '' and x!=header)) \
	.map(lambda x : x.split(",")) \
	.filter(lambda x : x[4] != 'NA') \
	.map(lambda x : ""+x[0]+","+x[1]+","+x[2]+","+x[4]+","+x[8]+","+x[9]+","+x[14]+","+x[16]+","+x[17]+"") 

	f2m = f2.filter(lambda x : (x != '' and x!=header)) \
	.map(lambda x : x.split(",")) \
	.filter(lambda x : x[4] != 'NA') \
	.map(lambda x : ""+x[0]+","+x[1]+","+x[2]+","+x[4]+","+x[8]+","+x[9]+","+x[14]+","+x[16]+","+x[17]+"") 

	f3m = f3.filter(lambda x : (x != '' and x!=header)) \
	.map(lambda x : x.split(",")) \
	.filter(lambda x : x[4] != 'NA') \
	.map(lambda x : ""+x[0]+","+x[1]+","+x[2]+","+x[4]+","+x[8]+","+x[9]+","+x[14]+","+x[16]+","+x[17]+"") 

	# Save data
	
	f1m.saveAsTextFile("flightdelays_clean/fldata1.csv")
	f2m.saveAsTextFile("flightdelays_clean/fldata2.csv")
        f3m.saveAsTextFile("flightdelays_clean/fldata3.csv")

	'''
	clean_final = (f1m.union(f2m)).union(f3m)
	clean_final.coalesce(1).saveAsTextFile("flightdelays_clean")
	'''

	# TASK 03 : calculates the number of rows of cleaned data
	f1_cnt = f1m.count()
	f2_cnt = f2m.count()
	f3_cnt = f3m.count()

	totalRows = f1_cnt + f2_cnt + f3_cnt
	temp = ['Total number of cleaned records are :',str(totalRows)]
	sc.parallelize(temp).saveAsTextFile("cleaned_total")

	# Calculating flights arriving late on denver airport
	den1 = f1m.map(lambda x : x.split(",")).filter( lambda x : x[8] == "DEN").map(lambda x : ",".join(x))
	den2 = f2m.map(lambda x : x.split(",")).filter( lambda x : x[8] == "DEN").map(lambda x : ",".join(x))
	den3 = f1m.map(lambda x : x.split(",")).filter( lambda x : x[8] == "DEN").map(lambda x : ",".join(x))
	den = (den1.union(den2)).union(den3)
	den.coalesce(1).saveAsTextFile("denver_total")

	den_60 = den.map(lambda x : x.split(",")) \
	.filter(lambda x : (x[6] != '' and x[6] != 'NA')) \
	.filter(lambda x : (float(x[6]) > 59)) \
	.map(lambda x : ",".join(x))

	den_60.coalesce(1).saveAsTextFile("denver_late")
	 

	'''
	to access hive tables from spark shell, copied usr/lib/hive/conf/hive-site.xml to usr/lib/spark/conf/
	'''

	hc = HiveContext(sc)
	
	fdata = hc.table("default.flightdelays") 
	rfdata = fdata.registerTempTable("fdata") 
	late = hc.sql("select * from fdata where arrdelay > 0 order by arrdelay DESC")
	
	# Save delayed flights data. ** notice DATAFRAME.rdd below** 	
	late.rdd.map(lambda x : str(x.year)+","+str(x.month)+","+str(x.dayofmonth)+","+str(x.deptime)+","+ \
	x.uniquecarrier +","+str(x.flightnum)+","+str(x.arrdelay)+","+x.origin+","+x.dest) \
	.saveAsTextFile("flightdelays_non0")

	#Compute the average arrdelay of flights landing in Denver (dest equals "DEN")
	hc.sql("select avg(arrdelay)  from fdata where dest='DEN'").show()
	#Compute the average arrdelay of flights where the origin is LAX and the dest is SFO
	hc.sql("select avg(arrdelay)  from fdata where dest='SFO' and origin = 'LAX'").show()
	#Determine which dest airport had the highest average arrdelay
	hc.sql("select dest,avg(arrdelay) as arrdelay  from delayed group by dest order by arrdelay DESC").first()

	# create table	
	hc.sql(" \
	CREATE TABLE SFO_WEATHER ( \
	STATION_NAME STRING, \
	YEAR INT,\
	MONTH INT,\
	DAYOFMONTH INT, \
	PRECIPITATION INT, \
	TEMPERATURE_MAX INT, \
	TEMPERATURE_MIN INT)\
	ROW FORMAT DELIMITED \
	FIELDS TERMINATED BY ',' \
	LINES TERMINATED BY '\n' \
	STORED AS ORC").show()

	# since csv file records cant be loaded directly as ORC file, create a temporary table
	hc.sql(" \
	CREATE TABLE SFO_WEATHER_temp ( \
	STATION_NAME STRING, \
	YEAR INT,\
	MONTH INT,\
	DAYOFMONTH INT, \
	PRECIPITATION INT, \
	TEMPERATURE_MAX INT, \
	TEMPERATURE_MIN INT)\
	ROW FORMAT DELIMITED \
	FIELDS TERMINATED BY ',' \
	LINES TERMINATED BY '\n'").show()

	# Load temp table in SFO_WEATHER 
	hc.sql(" \
	LOAD DATA LOCAL INPATH 'file:///home/cloudera/Desktop/flight/sfo_weather.csv" INTO TABLE SFO_WEATHER_temp').show()

	# load table using query
	hc.sql("INSERT OVERWRITE table SFO_WEATHER SELECT * FROM SFO_WEATHER_TEMP").show()

	
	# DROP TEMP TABLE AFTER VERIFYING DATA LOADED CORRECTLY
	hc.sql("DROP TABLE SFO_WEATHER_TEMP")


	#task 09 

	hc.sql(" \
	CREATE TABLE WEATHER_PARTITIONED ( \
	STATION_NAME STRING, \
	DAYOFMONTH INT, \
	PRECIPITATION INT, \
	TEMPERATURE_MAX INT, \
	TEMPERATURE_MIN INT) \
	PARTITIONED BY (YEAR INT,MONTH INT) \
	ROW FORMAT DELIMITED \
	FIELDS TERMINATED BY ',' \
	LINES TERMINATED BY '\n' \
	STORED AS ORC ").show()
	
	hc.sql(" \
	INSERT OVERWRITE TABLE WEATHER_PARTITIONED PARTITION(YEAR=2008,MONTH=01) SELECT * FROM SFO_WEATHER WHERE YEAR = 2008 AND MONTH = 01").show()
	
	sc.stop()

